# 需求调研与规划阶段
本推理引擎打算支持 Transformer 模型中 Qwen 系列、Llama 系列，以及 Deepseek R1 的大语言推理服务。

## 现有推理引擎
[DeepSpeed Mii](https://github.com/deepspeedai/DeepSpeed-MII)、[llama.cpp](https://github.com/ggml-org/llama.cpp)、[vllm](https://github.com/vllm-project/vllm)、[ktransformers](https://github.com/kvcache-ai/ktransformers/blob/main/README_ZH.md)、[huggingface-TGI](https://github.com/huggingface/text-generation-inference)、[SGLang](https://github.com/sgl-project/sglang)、[LMDeploy](https://github.com/InternLM/lmdeploy)


## 技术调研
量化技术、并行计算、内存管理、平台适配、多节点、推理技术

## 调研结果
||量化技术|并行计算|内存管理|平台适配|多节点|推理技术|
|----|----|----|----|----|----|----|
|llama.cpp|T|可按照 layer、row 切分|offload|AMD、NVIDIA、NPU、MUSA|F||
|vllm|T|TP、PP 均分|Paged Attention、offload|AMD、NVIDIA、TPU|T|Chunked prefill、prefix caching|
|LMDeploy|T|TP|blocked KV cache|NVIDIA|T|prefix caching|
|SGLang|T|TP|paged attention|AMD、NVIDIA|T|chunked prefill|
|DeepSpeed Mii|F|TP|blocked KV cache|NVIDIA|F||
|huggingface-TGI|T|TP| Paged Attention|AMD、NVIDIA|F|Speculative decoding|



--

3. 核心模块开发与原型验证
算子与内核实现
从基本的线性代数算子开始实现（如矩阵乘法、卷积、归一化、激活函数等）。
针对目标硬件进行优化，如利用 SIMD 指令集、GPU 并行计算等。
设计可扩展的算子库，便于后续加入更多专用算子。
模型解析与转换工具
开发工具将主流框架（例如 PyTorch、TensorFlow）的模型转换为你推理引擎的专用格式。
实现图优化（算子融合、冗余节点剔除等）以提高运行效率。
原型测试与验证
构建最小化可用原型，验证数据流、算子执行和调度逻辑。
使用简单模型进行端到端测试，确保整体流程的正确性。

4. 性能优化与高级特性实现
硬件加速与并行优化
针对 GPU 开发高性能内核，并探索多线程、多核 CPU 的并行调度。
实现模型并行、数据并行或管道并行（根据模型规模选择合适的并行策略）。
模型量化与压缩
支持 FP16、INT8 等低精度计算，提高推理速度和降低内存占用。
研究剪枝、蒸馏等模型压缩技术，以便在边缘设备上高效运行。
算子融合与调度优化
设计自动化图优化算法，将多个连续算子融合，减少中间数据传输。
实现动态调度机制，依据输入规模自动调整批处理策略。

5. API 设计、封装与生态构建
高层 API 与开发者接口
提供 Python 绑定或 RESTful API，方便用户集成与调用。
编写详细文档、示例代码和教程，降低使用门槛。
监控、日志与错误处理
集成性能监控与日志记录系统，方便调试和后续优化。
构建异常处理和恢复机制，确保引擎的稳定性。

6. 测试、Benchmark 及持续集成
功能与性能测试
编写全面的单元测试、集成测试，确保各模块正确工作。
对标现有推理引擎，设计标准化的 Benchmark 测试，评估延迟、吞吐量和能耗等指标。
持续集成与自动化部署
建立 CI/CD 流水线，自动化构建、测试与打包。
使用容器化技术（如 Docker）方便部署与跨平台验证。

7. 部署、维护与迭代优化
部署与应用集成
针对不同环境（云端、边缘设备等）优化部署方案，确保资源利用率与响应速度。
与现有应用场景（如聊天机器人、搜索系统、推荐系统）进行深度集成。
性能监控与反馈机制
部署后持续监控性能指标，收集用户反馈与异常报告。
根据实际运行情况定期迭代更新，包括优化内核、更新调度策略、扩展新功能。
开源与社区支持（可选）
考虑开源部分或全部代码，吸引社区贡献，共同推动引擎的进步。
建立技术博客、论坛或讨论组，与同行分享优化经验和技术成果。
